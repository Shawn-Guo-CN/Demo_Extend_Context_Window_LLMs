block_size = 1024 # max seq length
hidden_dim = 3072
# GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency
vocab_size = 50304
n_layer = 12
n_head = 12
n_kv_head = 4
head_dim = 64
emb_dim = 768
dropout = 0.0
bias = false # true for GPT-2
rmsnorm_eps = 1e-6
sliding_window = -1
use_moe = false
max_batch_size = 8

[moe]
n_expert = 8
n_expert_per_token = 2

[rope]
theta = 1e4